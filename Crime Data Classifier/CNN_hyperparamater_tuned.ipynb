{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dc324e1",
   "metadata": {},
   "source": [
    "# London Crime CNN Classifier\n",
    "Predicting **Crime type** using TensorFlow/Keras Conv1D Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baba850f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 15:43:38.575972: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-15 15:43:39.239522: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-15 15:43:41.390267: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, BatchNormalization, Conv1D, GlobalMaxPooling1D, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow import keras\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bef6a874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "Num GPUs Available:  1\n",
      "GPU(s) configured: ['/physical_device:GPU:0']\n",
      "Compute dtype: float16\n",
      "Variable dtype: float32\n"
     ]
    }
   ],
   "source": [
    "# GPU Configuration\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Enable memory growth to prevent TensorFlow from allocating all GPU memory at once\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPU(s) configured: {[gpu.name for gpu in gpus]}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU found, running on CPU\")\n",
    "\n",
    "# Enable mixed precision for better GPU performance\n",
    "from tensorflow.keras import mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "print('Compute dtype: %s' % policy.compute_dtype)\n",
    "print('Variable dtype: %s' % policy.variable_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a46835",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9871ba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Dataset/mergedCRDataset.csv\", sep=\"\\t\", engine=\"python\", on_bad_lines=\"skip\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34f71f4",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e27c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "\n",
    "# Drop duplicates rows\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Drop the Crime ID.\n",
    "df.drop(columns=[\"Crime ID\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c506111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip column names\n",
    "# REASON: In some cases, when reading CSV files, extra spaces can be inadvertently added in the column names.\n",
    "# This can lead to issues when trying to access these columns later in the code, as the names won't match exactly.\n",
    "df.columns = df.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "923cd1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing geographic coordinates using median values grouped by location\n",
    "numeric_cols = ['Longitude', 'Latitude']\n",
    "group_col = 'LSOA name'   # adjust this to the most relevant geographic field\n",
    "\n",
    "# Calculate median coordinates within each geographic group\n",
    "group_medians = df.groupby(group_col)[numeric_cols].median()\n",
    "\n",
    "# Attach group medians to the dataframe\n",
    "df = df.merge(group_medians, left_on=group_col, right_index=True, how='left', suffixes=('', '_grp'))\n",
    "\n",
    "# Fill missing values using the median of the corresponding group\n",
    "for col in numeric_cols:\n",
    "    df[col] = df[col].fillna(df[f'{col}_grp'])\n",
    "\n",
    "# Fallback to global median if an entire group has missing coordinates\n",
    "global_medians = df[numeric_cols].median()\n",
    "df[numeric_cols] = df[numeric_cols].fillna(global_medians)\n",
    "\n",
    "# Remove helper columns containing group medians\n",
    "df.drop(columns=[f'{c}_grp' for c in numeric_cols], inplace=True)\n",
    "\n",
    "\n",
    "# Encode target values for classification\n",
    "target_col = 'Crime type'\n",
    "encoder = LabelEncoder()\n",
    "df['target_encoded'] = encoder.fit_transform(df[target_col])\n",
    "num_classes = df['target_encoded'].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9dd549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features: numeric and categorical\n",
    "numeric_cols = ['Longitude', 'Latitude']\n",
    "categorical_cols = ['Reported by', 'Falls within']   # safe one-hot features\n",
    "\n",
    "# One-hot encode selected categorical features\n",
    "df_encoded = pd.get_dummies(df[categorical_cols])\n",
    "\n",
    "# Concatenate numeric + categorical\n",
    "X = pd.concat([df[numeric_cols], df_encoded], axis=1).values\n",
    "\n",
    "# Target already label-encoded earlier\n",
    "y = to_categorical(df['target_encoded'], num_classes=num_classes)\n",
    "\n",
    "print(f\"Feature shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a385267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "X[:, :len(numeric_cols)] = scaler.fit_transform(X[:, :len(numeric_cols)])\n",
    "\n",
    "X = X.astype(\"float32\")  # ensure proper dtype\n",
    "\n",
    "# Reshape for Conv1D: (samples, timesteps, features)\n",
    "# Treat each feature as a timestep for Conv1D processing\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "print(f\"Reshaped X for Conv1D: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecf214d",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "065846b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d5b8d6",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618cb2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU-Optimized Conv1D Model\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    \n",
    "    # First Conv1D block\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Second Conv1D block\n",
    "    Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Third Conv1D block\n",
    "    Conv1D(filters=256, kernel_size=3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    # Global pooling to reduce dimensions\n",
    "    GlobalMaxPooling1D(),\n",
    "    \n",
    "    # Dense layers\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Output layer with float32 dtype for numerical stability\n",
    "    Dense(y.shape[1], activation='softmax', dtype='float32')\n",
    "])\n",
    "\n",
    "# Compile with optimized settings for GPU\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Create Model directory if it doesn't exist\n",
    "save_path = \"Model\"\n",
    "os.makedirs(save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9513f5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9c43281",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9dd215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with GPU-optimized settings\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=512,  # Larger batch size for better GPU utilization\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(os.path.join(save_path, \"network.h5\"))\n",
    "print(f\"Model saved to {save_path}/network.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60fe135",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions (will give a probability distribution)\n",
    "pred_hot = model.predict(X_test)\n",
    "#now pick the most likely outcome\n",
    "pred = np.argmax(pred_hot,axis=1)\n",
    "y_compare = np.argmax(y_test,axis=1) \n",
    "#calculate accuracy\n",
    "score = metrics.accuracy_score(y_compare, pred)\n",
    "\n",
    "print(\"Accuracy score: {}\".format(score))\n",
    "\n",
    "print(pred_hot[:5])\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c282b83",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82b50b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(y_true_classes, y_pred_classes, target_names=encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e7b771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "model.eval()\n",
    "all_preds, all_true = [], []\n",
    "all_probs = []\n",
    "with torch.no_grad():\n",
    "    for bx, by in test_loader:\n",
    "        bx = bx.to(device)\n",
    "        logits = model(bx)\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_probs.extend(probs)\n",
    "        all_true.extend(by.numpy())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_probs = np.array(all_probs)\n",
    "all_true = np.array(all_true)\n",
    "\n",
    "#\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPREHENSIVE MODEL EVALUATION METRICS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n Classification Report:\")\n",
    "print(classification_report(all_true, all_preds))\n",
    "\n",
    "# 1. Macro F1 Score\n",
    "from sklearn.metrics import f1_score\n",
    "macro_f1 = f1_score(all_true, all_preds, average='macro', zero_division=0)\n",
    "weighted_f1 = f1_score(all_true, all_preds, average='weighted', zero_division=0)\n",
    "print(f\"\\n 1. F1 SCORES:\")\n",
    "print(f\"   Macro F1 (treats all classes equally): {macro_f1:.4f}\")\n",
    "print(f\"   Weighted F1 (accounts for class imbalance): {weighted_f1:.4f}\")\n",
    "\n",
    "# 2. Macro Recall (Sensitivity per class)\n",
    "from sklearn.metrics import recall_score\n",
    "macro_recall = recall_score(all_true, all_preds, average='macro', zero_division=0)\n",
    "weighted_recall = recall_score(all_true, all_preds, average='weighted', zero_division=0)\n",
    "per_class_recall = recall_score(all_true, all_preds, average=None, zero_division=0)\n",
    "print(f\"\\n 2. RECALL SCORES (Detection Rate):\")\n",
    "print(f\"   Macro Recall: {macro_recall:.4f}\")\n",
    "print(f\"   Weighted Recall: {weighted_recall:.4f}\")\n",
    "print(f\"   Per-class Recall:\")\n",
    "for i, recall in enumerate(per_class_recall):\n",
    "    print(f\"      Class {i}: {recall:.4f}\")\n",
    "\n",
    "# 3. Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(all_true, all_preds)\n",
    "print(f\"\\n 3. Confusion Matrix\")\n",
    "print(cm)\n",
    "\n",
    "# 4. AUROC / ROC-AUC (macro averaged)\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "try:\n",
    "    # For multi-class: use macro and weighted averaging\n",
    "    if num_classes == 2:\n",
    "        macro_auc = roc_auc_score(all_true, all_probs[:, 1])\n",
    "    else:\n",
    "        macro_auc = roc_auc_score(all_true, all_probs, multi_class='ovr', average='macro')\n",
    "        weighted_auc = roc_auc_score(all_true, all_probs, multi_class='ovr', average='weighted')\n",
    "    \n",
    "    print(f\"\\n 4. AUROC / ROC-AUC Score:\")\n",
    "    if num_classes == 2:\n",
    "        print(f\"   Binary AUC: {macro_auc:.4f}\")\n",
    "    else:\n",
    "        print(f\"   Macro AUC (One-vs-Rest): {macro_auc:.4f}\")\n",
    "        print(f\"   Weighted AUC: {weighted_auc:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n 4. AUROC / ROC-AUC:\")\n",
    "    print(f\"   Could not compute AUC: {str(e)}\")\n",
    "\n",
    "# Overall Accuracy\n",
    "accuracy = accuracy_score(all_true, all_preds)\n",
    "print(f\"\\n Overall Accuracy: {accuracy:.4f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb923fc",
   "metadata": {},
   "source": [
    "Data Analysis/Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6578eab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with multiple subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Confusion Matrix Heatmap\n",
    "ax1 = axes[0, 0]\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1, cbar_kws={'label': 'Count'})\n",
    "ax1.set_title('Confusion Matrix (Raw Counts)', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('True Label', fontsize=12)\n",
    "ax1.set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "# 2. Normalized Confusion Matrix\n",
    "ax2 = axes[0, 1]\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='RdYlGn', ax=ax2, cbar_kws={'label': 'Recall %'})\n",
    "ax2.set_title('Confusion Matrix (Normalized by True Label)', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('True Label', fontsize=12)\n",
    "ax2.set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "# 3. Training Loss and Validation Accuracy\n",
    "ax3 = axes[1, 0]\n",
    "epochs_range = range(1, len(loss_across_epochs) + 1)\n",
    "ax3_twin = ax3.twinx()\n",
    "line1 = ax3.plot(epochs_range, loss_across_epochs, 'b-o', label='Training Loss', linewidth=2, markersize=6)\n",
    "line2 = ax3_twin.plot(epochs_range, val_accuracy_across_epochs, 'g-s', label='Validation Accuracy', linewidth=2, markersize=6)\n",
    "ax3.set_xlabel('Epoch', fontsize=12)\n",
    "ax3.set_ylabel('Loss', fontsize=12, color='b')\n",
    "ax3_twin.set_ylabel('Accuracy', fontsize=12, color='g')\n",
    "ax3.tick_params(axis='y', labelcolor='b')\n",
    "ax3_twin.tick_params(axis='y', labelcolor='g')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_title('Training Loss vs Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Combine legends\n",
    "lines = line1 + line2\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax3.legend(lines, labels, loc='upper left', fontsize=10)\n",
    "\n",
    "# 4. Per-Class Recall Bar Chart\n",
    "ax4 = axes[1, 1]\n",
    "class_names = [f'Class {i}' for i in range(len(per_class_recall))]\n",
    "colors = ['green' if r > 0.5 else 'orange' if r > 0.3 else 'red' for r in per_class_recall]\n",
    "bars = ax4.bar(class_names, per_class_recall, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "ax4.axhline(y=macro_recall, color='r', linestyle='--', linewidth=2, label=f'Macro Recall: {macro_recall:.4f}')\n",
    "ax4.set_ylabel('Recall', fontsize=12)\n",
    "ax4.set_title('Per-Class Recall (Detection Rate)', fontsize=14, fontweight='bold')\n",
    "ax4.set_ylim([0, 1])\n",
    "ax4.legend(fontsize=10)\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, recall in zip(bars, per_class_recall):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{recall:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_evaluation_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5194c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Additional metrics visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 1. F1 Scores Comparison\n",
    "ax1 = axes[0]\n",
    "metrics = ['Macro F1', 'Weighted F1']\n",
    "scores = [macro_f1, weighted_f1]\n",
    "colors_metrics = ['#FF6B6B', '#4ECDC4']\n",
    "bars = ax1.bar(metrics, scores, color=colors_metrics, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax1.set_ylabel('F1 Score', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('F1 Scores Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, score in zip(bars, scores):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{score:.4f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 2. Macro Metrics Summary\n",
    "ax2 = axes[1]\n",
    "macro_precision = precision_score(all_true, all_preds, average='macro', zero_division=0)\n",
    "summary_metrics = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
    "summary_values = [accuracy, macro_precision, macro_recall, macro_f1]\n",
    "colors_summary = ['#95E1D3', '#F38181', '#AA96DA', '#FCBAD3']\n",
    "\n",
    "bars = ax2.barh(summary_metrics, summary_values, color=colors_summary, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax2.set_xlabel('Score', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Macro-Averaged Metrics Summary', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlim([0, 1])\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "for bar, value in zip(bars, summary_values):\n",
    "    width = bar.get_width()\n",
    "    ax2.text(width, bar.get_y() + bar.get_height()/2.,\n",
    "             f'{value:.4f}', ha='left', va='center', fontsize=11, fontweight='bold', \n",
    "             bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('metrics_summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb7153b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ROC-AUC Curves (One-vs-Rest for multi-class)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# 1. ROC Curve (One-vs-Rest)\n",
    "ax1 = axes[0]\n",
    "if num_classes > 2:\n",
    "    # Multi-class: use label binarization\n",
    "    all_true_bin = label_binarize(all_true, classes=range(num_classes))\n",
    "    \n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, num_classes))\n",
    "    auc_scores = []\n",
    "    \n",
    "    for i in range(min(num_classes, 5)):  # Limit to 5 classes for clarity\n",
    "        fpr, tpr, _ = roc_curve(all_true_bin[:, i], all_probs[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        auc_scores.append(roc_auc)\n",
    "        ax1.plot(fpr, tpr, color=colors[i], lw=2, label=f'Class {i} (AUC = {roc_auc:.3f})')\n",
    "    \n",
    "    # Micro-average\n",
    "    fpr, tpr, _ = roc_curve(all_true_bin.ravel(), all_probs.ravel())\n",
    "    roc_auc_micro = auc(fpr, tpr)\n",
    "    ax1.plot(fpr, tpr, color='deeppink', lw=3, linestyle=':', label=f'Micro-average (AUC = {roc_auc_micro:.3f})')\n",
    "    \n",
    "    ax1.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
    "    ax1.set_xlim([0.0, 1.0])\n",
    "    ax1.set_ylim([0.0, 1.05])\n",
    "    ax1.set_xlabel('False Positive Rate', fontsize=11, fontweight='bold')\n",
    "    ax1.set_ylabel('True Positive Rate', fontsize=11, fontweight='bold')\n",
    "    ax1.set_title('ROC Curves (One-vs-Rest) - Top 5 Classes', fontsize=13, fontweight='bold')\n",
    "    ax1.legend(loc=\"lower right\", fontsize=9)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "else:\n",
    "    # Binary classification\n",
    "    fpr, tpr, _ = roc_curve(all_true, all_probs[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax1.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "    ax1.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
    "    ax1.set_xlim([0.0, 1.0])\n",
    "    ax1.set_ylim([0.0, 1.05])\n",
    "    ax1.set_xlabel('False Positive Rate', fontsize=11, fontweight='bold')\n",
    "    ax1.set_ylabel('True Positive Rate', fontsize=11, fontweight='bold')\n",
    "    ax1.set_title('ROC Curve', fontsize=13, fontweight='bold')\n",
    "    ax1.legend(loc=\"lower right\", fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Per-Class Metrics Heatmap\n",
    "ax2 = axes[1]\n",
    "precision, recall, f1, support = precision_recall_fscore_support(all_true, all_preds, \n",
    "                                                                   average=None, zero_division=0)\n",
    "\n",
    "metrics_data = np.array([precision, recall, f1]).T\n",
    "im = ax2.imshow(metrics_data, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "\n",
    "# Set ticks and labels\n",
    "ax2.set_xticks([0, 1, 2])\n",
    "ax2.set_xticklabels(['Precision', 'Recall', 'F1'], fontsize=11, fontweight='bold')\n",
    "ax2.set_yticks(range(min(len(precision), 10)))\n",
    "ax2.set_yticklabels([f'Class {i}' for i in range(min(len(precision), 10))], fontsize=10)\n",
    "ax2.set_title('Per-Class Metrics Heatmap (Top 10 Classes)', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(min(len(precision), 10)):\n",
    "    for j in range(3):\n",
    "        text = ax2.text(j, i, f'{metrics_data[i, j]:.2f}',\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontsize=9, fontweight='bold')\n",
    "\n",
    "cbar = plt.colorbar(im, ax=ax2)\n",
    "cbar.set_label('Score', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_and_metrics_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
