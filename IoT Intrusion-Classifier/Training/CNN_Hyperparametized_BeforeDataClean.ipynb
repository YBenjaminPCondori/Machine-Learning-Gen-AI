{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "<h1 style=\"text-align:center; font-size:46px; font-weight:800; margin-bottom:0;\">\n",
    "Cybersecurity - IoT Intrusion Dataset\n",
    "</h1>\n",
    "<p style=\"text-align:center; font-size:22px; color:gray; margin-top:5px;\">\n",
    "Conv1D Neural Network for Time Series Analysis\n",
    "</p>\n",
    "<br>\n",
    "<p style=\"text-align:center;\">\n",
    "  <img src=\"img/ai.jpg\" style=\"width:85%; max-width:900px; border-radius:10px; box-shadow:0px 0px 12px rgba(0,0,0,0.25);\">\n",
    "</p>\n",
    "<br>\n",
    "<p style=\"text-align:center; font-size:16px; color:gray;\">\n",
    "Environment & Tools: Python â€¢ PyTorch â€¢ NumPy â€¢ Pandas â€¢ Scikit-Learn â€¢ Matplotlib\n",
    "</p>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used in this analysis is publicly available from an open-source repository on Kaggle.  \n",
    "You can access it here: [insert URL].\n",
    "\n",
    "We will be using a **1D Convolutional Neural Network (Conv1D CNN)** to perform **time-series analysis** on the dataset.\n",
    "Conv1D is particularly suited for sequential data, as it can capture temporal patterns across the input sequence of features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h2 style=\"text-align:center; font-weight:700;\">ðŸ“š Libraries Used in This Project</h2>\n",
    "<p style=\"text-align:center; color:gray; font-size:15px;\">Core tools used for data processing, modeling, and visualization.</p>\n",
    "<br>\n",
    "<table style=\"margin-left:auto; margin-right:auto; text-align:center;\">\n",
    "  <tr>\n",
    "    <td><img src=\"img/numpy.jpg\" width=\"60\"><br><strong>NumPy</strong><br><span style=\"color:gray; font-size:13px;\">Numerical<br>Computing</span></td>\n",
    "    <td><img src=\"img/pandas.png\" width=\"60\"><br><strong>Pandas</strong><br><span style=\"color:gray; font-size:13px;\">Data<br>Manipulation</span></td>\n",
    "    <td><img src=\"img/pytorch.jpg\" width=\"100\"><br><strong>PyTorch</strong><br><span style=\"color:gray; font-size:13px;\">Deep<br>Learning</span></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"img/scikitlearn.png\" width=\"60\"><br><strong>Scikit-Learn</strong><br><span style=\"color:gray; font-size:13px;\">ML Tools<br>& Metrics</span></td>\n",
    "    <td><img src=\"img/matplotlib.jpg\" width=\"90\"><br><strong>Matplotlib</strong><br><span style=\"color:gray; font-size:13px;\">Visualization</span></td>\n",
    "  </tr>\n",
    "</table>\n",
    "<br>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Thu Oct 23 18:45:00 2025\n",
    "\n",
    "@author: ybenjaminpcondori\n",
    "\"\"\"\n",
    "\n",
    "# System & OS utilities\n",
    "import os\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_fscore_support\n",
    "from sklearn.metrics import (classification_report, f1_score, recall_score,\n",
    "                             confusion_matrix, roc_auc_score, accuracy_score)\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Importing Seaborn for enhanced visualizations\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting for Presentation purposes on Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaring Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Neural Network Definition\n",
    "class Convolutional_Neural_Network(nn.Module):\n",
    "    def __init__(self, num_classes, num_features):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutional feature extractor\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "\n",
    "        # Second convolutional block\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        # Third convolutional block\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "\n",
    "        # Regularisation inside convolutional blocks\n",
    "        self.conv_dropout = nn.Dropout1d(0.2)\n",
    "\n",
    "        # Pooling\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # Activation\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Determine feature size dynamically\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 1, num_features)\n",
    "            d = self._extract_features(dummy)\n",
    "            self.flatten_dim = d.shape[1]\n",
    "\n",
    "        # Reduced fully connected head (less memorisation)\n",
    "        self.fc1 = nn.Linear(self.flatten_dim, 64)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.out = nn.Linear(64, num_classes)\n",
    "\n",
    "    # Feature extraction method\n",
    "    def _extract_features(self, x):\n",
    "\n",
    "        # Convolutional layers with batch norm, ReLU, dropout, and pooling\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.conv_dropout(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Second convolutional block\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.conv_dropout(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Third convolutional block\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.global_pool(x)\n",
    "\n",
    "        x = x.squeeze(-1)  # (batch, channels)\n",
    "        return x\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # (batch, 1, features)\n",
    "        x = self._extract_features(x)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        return self.out(x)\n",
    "\n",
    "# Binary counting helper class\n",
    "class DataPreprocessing:\n",
    "\n",
    "    @staticmethod\n",
    "    def count_binary(series):\n",
    "        counts = series.value_counts()\n",
    "        return pd.Series({'0': counts.get(0, 0), '1': counts.get(1, 0)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the CSV Dataset using Pandas\n",
    "df = pd.read_csv(\"IoT_Intrusion.csv\")\n",
    "\n",
    "# Sripping whitespace and converting to lowercase\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "# Displaying the first few rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaration of Columns: Network Protocols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing binary columns\n",
    "protocol_columns = ['http','https','dns','telnet','smtp','ssh','irc','tcp','udp','dhcp','arp','icmp','ipv','llc']\n",
    "for col in protocol_columns:\n",
    "    _ = df[col].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance: Number of instances of 0/1 in each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting binary values in specified columns\n",
    "counts_df = df[protocol_columns].apply(DataPreprocessing.count_binary)\n",
    "print(counts_df)\n",
    "\n",
    "# Printing label statistics\n",
    "print(\"Binary label statistics:\")\n",
    "print(\"Unique label count:\", df['label'].nunique())\n",
    "print(\"Unique labels:\", df['label'].unique())\n",
    "print(\"Label counts:\\n\", df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caclulating Uniques Values, important for identifying rows that don't contribute to the feature prediction/classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Calculating unique values\n",
    "# -----------------------------\n",
    "\n",
    "print(\"\\nUnique Value Counts\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "protocol_columns_nunique = df[protocol_columns].nunique()\n",
    "print(protocol_columns_nunique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaration of Columns: Signal Processing Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to analyze\n",
    "other_columns = [\n",
    "    \"flow_duration\",\n",
    "    \"header_length\",\n",
    "    \"protocol_type\",\n",
    "    \"duration\",\n",
    "    \"rate\",\n",
    "    \"srate\",\n",
    "    \"drate\",\n",
    "    \"fin_flag_number\",\n",
    "    \"syn_flag_number\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null value analysis\n",
    "print(\"Null Value Analysis for Other Columns\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for col in other_columns:\n",
    "    null_count = df[col].isnull().sum()\n",
    "    null_ratio = df[col].isnull().mean()\n",
    "\n",
    "    print(f\"{col:20s} | nulls: {null_count:8d} | ratio: {null_ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Calculating unique values\n",
    "# -----------------------------\n",
    "\n",
    "print(\"\\nUnique Value Counts\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "other_columns_nunique = df[other_columns].nunique()\n",
    "print(other_columns_nunique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaration of Columns: Numeric Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with different names\n",
    "different_columns = [\n",
    "    \"tot size\",\n",
    "    \"magnitue\",\n",
    "    \"iat\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Renaming columns for consistency\n",
    "df = df.rename(columns = {\n",
    "    \"tot size\": \"total_size\",\n",
    "    \"magnitue\": \"magnitude\",\n",
    "    \"iat\": \"inter_arrival_time\",\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null value analysis\n",
    "print(\"Null Value Analysis for Different Columns\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for col in different_columns:\n",
    "    null_count = df[col].isnull().sum()\n",
    "    null_ratio = df[col].isnull().mean()\n",
    "\n",
    "    print(f\"{col:20s} | nulls: {null_count:8d} | ratio: {null_ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caclulation Uniques Values, important for identifying rows that don't contribute to the feature prediction/classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Calculating unique values\n",
    "# -----------------------------\n",
    "print(\"\\nUnique Value Counts\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "different_columns_nunique = df[different_columns].nunique()\n",
    "print(different_columns_nunique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting rows with identical features, but different target feature (Label)\n",
    "\n",
    "# Identify feature columns (exclude target)\n",
    "feature_cols = df.columns.difference(['label'])\n",
    "\n",
    "print(\"Detecting feature-identical rows with multiple target labels\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Retrieve all rows involved in label conflicts\n",
    "print(df_conflicts.sort_values(feature_cols.tolist()))\n",
    "\n",
    "\n",
    "# Detect feature-identical rows with multiple target labels\n",
    "label_variation = (\n",
    "    df\n",
    "    .groupby(feature_cols)['label']\n",
    "    .nunique()\n",
    "    .reset_index(name='n_labels')\n",
    ")\n",
    "\n",
    "# Keep only conflicting feature patterns\n",
    "conflicting_patterns = label_variation[label_variation['n_labels'] > 1]\n",
    "\n",
    "# Retrieve all rows involved in label conflicts\n",
    "df_conflicts = df.merge(\n",
    "    conflicting_patterns[feature_cols],\n",
    "    on=feature_cols,\n",
    "    how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Detect constant columns across the FULL dataset (excluding label)\n",
    "feature_cols = df.columns.difference(['label'])\n",
    "\n",
    "global_nunique = df[feature_cols].nunique()\n",
    "\n",
    "constant_cols = global_nunique[global_nunique == 1].index.tolist()\n",
    "\n",
    "print(\"Dropping constant columns:\", constant_cols)\n",
    "\n",
    "df.drop(columns=constant_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify Features and Target Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "df['label_encoded'] = encoder.fit_transform(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Class to Attack Label Mapping \n",
    "\n",
    "id_to_attack = dict(enumerate(encoder.classes_))\n",
    "attack_to_id = {label: idx for idx, label in id_to_attack.items()}\n",
    "\n",
    "# Displaying the class ID to attack type mapping\n",
    "print(\"\\nClass ID â†’ Attack Type mapping:\\n\")\n",
    "for k, v in id_to_attack.items():\n",
    "    print(f\"Class {k}: {v}\")\n",
    " \n",
    "# Saving the mapping to a CSV file\n",
    "mapping_df = pd.DataFrame({\n",
    "    \"Class ID\": list(id_to_attack.keys()),\n",
    "    \"Attack Type\": list(id_to_attack.values())\n",
    "})\n",
    "\n",
    "# Saving the mapping to a CSV file\n",
    "mapping_df.to_csv(\"class_label_mapping.csv\", index=False)\n",
    "print(\"\\n[âœ“] Saved class_label_mapping.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values\n",
    "target = 'label_encoded'\n",
    "features = [c for c in df.columns if c not in ['label', 'label_encoded']]\n",
    "\n",
    "# Fill missing values in\n",
    "numeric_features = df[features].select_dtypes(include=np.number).columns\n",
    "df[numeric_features] = df[numeric_features].fillna(df[numeric_features].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of X and Y values\n",
    "X = df[features].values\n",
    "y = df[target].values\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Splitting the dataset into training and testing sets with stratification\n",
    "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_train_np = scaler.fit_transform(X_train_np)\n",
    "X_test_np  = scaler.transform(X_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to PyTorch tensors\n",
    "X_train = torch.tensor(X_train_np, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test_np, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train_np, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test_np, dtype=torch.long)\n",
    "\n",
    "# Displaying the shapes of the training and testing tensors\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataLoader based on batch size\n",
    "batch_size = 512\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model instantiation\n",
    "num_classes = len(df[target].unique())\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "\n",
    "# Convolutional Neural Network model\n",
    "model = Convolutional_Neural_Network(num_classes, num_features).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function Crossentropy Loss\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer hyperparameters\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4  # optional but recommended\n",
    ")\n",
    "\n",
    "# Learning rate scheduler to reduce LR on plateau\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-5,\n",
    "    verbose=True\n",
    ")\n",
    " \n",
    "# Training hyperparameters\n",
    "num_epochs = 50\n",
    "\n",
    "# Early Stopping \n",
    "best_val_loss = float(\"inf\")\n",
    "patience = 5          # stop after 5 bad epochs\n",
    "patience_ctr = 0\n",
    "min_delta = 1e-4\n",
    "best_model_path = \"best_model.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with validation and learning rate scheduling\n",
    "for epoch in range(num_epochs):\n",
    "    # -------- Train --------\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # For looping through training batches\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "        # Training using backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch_x)\n",
    "        loss = loss_function(logits, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss (sum over samples)\n",
    "        total_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "        # Accumulate accuracy\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == batch_y).sum().item()\n",
    "        total += batch_y.size(0)\n",
    "\n",
    "    # Calculating average training loss/accuracy\n",
    "    train_loss = total_loss / len(train_loader.dataset)\n",
    "    train_acc = correct / total if total > 0 else 0.0\n",
    "\n",
    "    # -------- Validation (using test_loader as validation set) --------\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for bx, by in test_loader:\n",
    "            bx, by = bx.to(device), by.to(device)\n",
    "            logits = model(bx)\n",
    "            val_loss += loss_function(logits, by).item() * bx.size(0)\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            val_correct += (preds == by).sum().item()\n",
    "            val_total += by.size(0)\n",
    "\n",
    "    # Calculating average validation loss/accuracy\n",
    "    val_loss /= len(test_loader.dataset)\n",
    "    val_acc = val_correct / val_total if val_total > 0 else 0.0\n",
    "\n",
    "    # Scheduler step\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Printing Training progress\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "        f\"Train Acc: {train_acc:.4f} | \"\n",
    "        f\"Train Loss: {train_loss:.4f} | \"\n",
    "        f\"Val Acc: {val_acc:.4f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f} | \"\n",
    "        f\"LR: {optimizer.param_groups[0]['lr']:.2e}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib global settings\n",
    "mpl.rcParams.update({\n",
    "    # Resolution\n",
    "    \"figure.dpi\": 150,\n",
    "    \"savefig.dpi\": 300,\n",
    "\n",
    "    # Font (IEEE/Springer safe)\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Times New Roman\", \"Times\", \"DejaVu Serif\"],\n",
    "\n",
    "    # Font sizes\n",
    "    \"font.size\": 10,\n",
    "    \"axes.titlesize\": 12,\n",
    "    \"axes.titleweight\": \"bold\",\n",
    "    \"axes.labelsize\": 11,\n",
    "    \"axes.labelweight\": \"bold\",\n",
    "\n",
    "    \"xtick.labelsize\": 9,\n",
    "    \"ytick.labelsize\": 9,\n",
    "\n",
    "    \"legend.fontsize\": 9,\n",
    "    \"legend.frameon\": False,\n",
    "\n",
    "    # Clean axes\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.spines.right\": False,\n",
    "    \"axes.grid\": False,\n",
    "\n",
    "    # Line widths\n",
    "    \"lines.linewidth\": 2,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluation metrics with attack names \n",
    "\n",
    "\n",
    "\n",
    "def evaluate_with_names(all_true, all_preds, all_probs, id_to_attack):\n",
    "    num_classes = len(id_to_attack)\n",
    "    class_names = [id_to_attack[i] for i in range(num_classes)]\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"MODEL EVALUATION METRICS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(\n",
    "        all_true,\n",
    "        all_preds,\n",
    "        target_names=class_names,\n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "    macro_f1 = f1_score(all_true, all_preds, average='macro', zero_division=0)\n",
    "    weighted_f1 = f1_score(all_true, all_preds, average='weighted', zero_division=0)\n",
    "\n",
    "    print(\"\\nF1 Scores:\")\n",
    "    print(f\"  Macro F1 Score:     {macro_f1:.4f}\")\n",
    "    print(f\"  Weighted F1 Score:  {weighted_f1:.4f}\")\n",
    "\n",
    "    macro_recall = recall_score(all_true, all_preds, average='macro', zero_division=0)\n",
    "    weighted_recall = recall_score(all_true, all_preds, average='weighted', zero_division=0)\n",
    "    per_class_recall = recall_score(all_true, all_preds, average=None, zero_division=0)\n",
    "\n",
    "    print(\"\\nRecall Scores:\")\n",
    "    print(f\"  Macro Recall:     {macro_recall:.4f}\")\n",
    "    print(f\"  Weighted Recall:  {weighted_recall:.4f}\")\n",
    "    print(\"  Per-class Recall:\")\n",
    "    for i, r in enumerate(per_class_recall):\n",
    "        print(f\"    {id_to_attack[i]}: {r:.4f}\")\n",
    "\n",
    "    cm = confusion_matrix(all_true, all_preds)\n",
    "    accuracy = accuracy_score(all_true, all_preds)\n",
    "\n",
    "    print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    return {\n",
    "        'macro_f1': macro_f1,\n",
    "        'weighted_f1': weighted_f1,\n",
    "        'macro_recall': macro_recall,\n",
    "        'weighted_recall': weighted_recall,\n",
    "        'accuracy': accuracy,\n",
    "        'confusion_matrix': cm\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Analysis / Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ROC-AUC curves and metrics heatmap with attack names\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_roc_and_heatmap(all_true, all_probs, id_to_attack,\n",
    "                          max_classes=5,\n",
    "                          save_path='roc_and_metrics_heatmap.png'):\n",
    "    num_classes = len(id_to_attack)\n",
    "    class_names = [id_to_attack[i] for i in range(num_classes)]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # ROC curves (OvR)\n",
    "    all_true_bin = label_binarize(all_true, classes=range(num_classes))\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, num_classes))\n",
    "\n",
    "    for i in range(min(num_classes, max_classes)):\n",
    "        fpr, tpr, _ = roc_curve(all_true_bin[:, i], all_probs[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        axes[0].plot(fpr, tpr, lw=2,\n",
    "                     label=f'{class_names[i]} (AUC={roc_auc:.3f})')\n",
    "\n",
    "    axes[0].plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    axes[0].set_xlabel('False Positive Rate')\n",
    "    axes[0].set_ylabel('True Positive Rate')\n",
    "    axes[0].set_title('ROC Curves (One-vs-Rest)')\n",
    "    axes[0].legend(fontsize=8)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Metrics heatmap\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_true, np.argmax(all_probs, axis=1), average=None, zero_division=0\n",
    "    )\n",
    "    metrics_data = np.vstack([precision, recall, f1]).T\n",
    "\n",
    "    im = axes[1].imshow(metrics_data[:max_classes], cmap='RdYlGn', vmin=0, vmax=1)\n",
    "    axes[1].set_xticks([0, 1, 2])\n",
    "    axes[1].set_xticklabels(['Precision', 'Recall', 'F1'])\n",
    "    axes[1].set_yticks(range(min(num_classes, max_classes)))\n",
    "    axes[1].set_yticklabels(class_names[:max_classes])\n",
    "    axes[1].set_title('Per-Class Metrics Heatmap')\n",
    "\n",
    "    plt.colorbar(im, ax=axes[1])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering all predictions and probabilities\n",
    "# Additional metrics visualization\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 1. F1 Scores Comparison\n",
    "ax1 = axes[0]\n",
    "metrics = ['Macro F1', 'Weighted F1']\n",
    "scores = [macro_f1, weighted_f1]\n",
    "colors_metrics = ['#FF6B6B', '#4ECDC4']\n",
    "bars = ax1.bar(metrics, scores, color=colors_metrics, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax1.set_ylabel('F1 Score', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('F1 Scores Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, score in zip(bars, scores):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{score:.4f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 2. Macro Metrics Summary\n",
    "ax2 = axes[1]\n",
    "macro_precision = precision_score(all_true, all_preds, average='macro', zero_division=0)\n",
    "summary_metrics = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
    "summary_values = [accuracy, macro_precision, macro_recall, macro_f1]\n",
    "colors_summary = ['#95E1D3', '#F38181', '#AA96DA', '#FCBAD3']\n",
    "\n",
    "bars = ax2.barh(summary_metrics, summary_values, color=colors_summary, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax2.set_xlabel('Score', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Macro-Averaged Metrics Summary', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlim([0, 1]) \n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "for bar, value in zip(bars, summary_values):\n",
    "    width = bar.get_width()\n",
    "    ax2.text(width, bar.get_y() + bar.get_height()/2.,\n",
    "             f'{value:.4f}', ha='left', va='center', fontsize=11, fontweight='bold', \n",
    "             bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('metrics_summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\" Metrics summary visualization saved as 'metrics_summary.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ROC-AUC Curves (One-vs-Rest for multi-class)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# 1. ROC Curve (One-vs-Rest)\n",
    "ax1 = axes[0]\n",
    "if num_classes > 2:\n",
    "    # Multi-class: use label binarization\n",
    "    all_true_bin = label_binarize(all_true, classes=range(num_classes))\n",
    "    \n",
    "    colors = plt.cm.Set3(np.linspace(0, 2, num_classes))\n",
    "    auc_scores = []\n",
    "    \n",
    "    for i in range(min(num_classes, 5)):  # Limit to 5 classes for clarity\n",
    "        fpr, tpr, _ = roc_curve(all_true_bin[:, i], all_probs[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        auc_scores.append(roc_auc)\n",
    "        ax1.plot(fpr, tpr, color=colors[i], lw=2, label=f'Class {i} (AUC = {roc_auc:.3f})')\n",
    "    \n",
    "    # Micro-average\n",
    "    fpr, tpr, _ = roc_curve(all_true_bin.ravel(), all_probs.ravel())\n",
    "    roc_auc_micro = auc(fpr, tpr)\n",
    "    ax1.plot(fpr, tpr, color='deeppink', lw=4, linestyle=':', label=f'Micro-average (AUC = {roc_auc_micro:.3f})')\n",
    "    \n",
    "    ax1.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
    "    ax1.set_xlim([0.0, 1.0])\n",
    "    ax1.set_ylim([0.0, 1.05])\n",
    "    ax1.set_xlabel('False Positive Rate', fontsize=11, fontweight='bold')\n",
    "    ax1.set_ylabel('True Positive Rate', fontsize=11, fontweight='bold')\n",
    "    ax1.set_title('ROC Curves (One-vs-Rest) - Top 5 Classes', fontsize=13, fontweight='bold')\n",
    "    ax1.legend(loc=\"lower right\", fontsize=9)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "else:\n",
    "    # Binary classification\n",
    "    fpr, tpr, _ = roc_curve(all_true, all_probs[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax1.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "    ax1.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
    "    ax1.set_xlim([0.0, 1.0])\n",
    "    ax1.set_ylim([0.0, 1.05])\n",
    "    ax1.set_xlabel('False Positive Rate', fontsize=11, fontweight='bold')\n",
    "    ax1.set_ylabel('True Positive Rate', fontsize=11, fontweight='bold')\n",
    "    ax1.set_title('ROC Curve', fontsize=13, fontweight='bold')\n",
    "    ax1.legend(loc=\"lower right\", fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Per-Class Metrics Heatmap\n",
    "ax2 = axes[1]\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision, recall, f1, support = precision_recall_fscore_support(all_true, all_preds, \n",
    "                                                                   average=None, zero_division=0)\n",
    "\n",
    "metrics_data = np.array([precision, recall, f1]).T\n",
    "im = ax2.imshow(metrics_data, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "\n",
    "# Set ticks and labels\n",
    "ax2.set_xticks([0, 1, 2])\n",
    "ax2.set_xticklabels(['Precision', 'Recall', 'F1'], fontsize=11, fontweight='bold')\n",
    "ax2.set_yticks(range(min(len(precision), 10)))\n",
    "ax2.set_yticklabels([f'Class {i}' for i in range(min(len(precision), 10))], fontsize=10)\n",
    "ax2.set_title('Per-Class Metrics Heatmap (Top 10 Classes)', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(min(len(precision), 10)):\n",
    "    for j in range(3):\n",
    "        text = ax2.text(j, i, f'{metrics_data[i, j]:.2f}',\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontsize=9, fontweight='bold')\n",
    "\n",
    "cbar = plt.colorbar(im, ax=ax2)\n",
    "cbar.set_label('Score', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_and_metrics_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\" ROC and metrics heatmap visualization saved as 'roc_and_metrics_heatmap.png'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
