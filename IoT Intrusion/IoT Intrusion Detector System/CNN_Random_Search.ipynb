{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "<h1 style=\"text-align:center; font-size:46px; font-weight:800; margin-bottom:0;\">\n",
    "Cybersecurity - IoT Intrusion Dataset\n",
    "</h1>\n",
    "<p style=\"text-align:center; font-size:22px; color:gray; margin-top:5px;\">\n",
    "Conv1D Neural Network for Time Series Analysis\n",
    "</p>\n",
    "<br>\n",
    "<p style=\"text-align:center;\">\n",
    "  <img src=\"img/ai.jpg\" style=\"width:85%; max-width:900px; border-radius:10px; box-shadow:0px 0px 12px rgba(0,0,0,0.25);\">\n",
    "</p>\n",
    "<br>\n",
    "<p style=\"text-align:center; font-size:16px; color:gray;\">\n",
    "Environment & Tools: Python â€¢ PyTorch â€¢ NumPy â€¢ Pandas â€¢ Scikit-Learn â€¢ Matplotlib\n",
    "</p>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used in this analysis is publicly available from an open-source repository on Kaggle.  \n",
    "You can access it here: [insert URL].\n",
    "\n",
    "We will be using a **1D Convolutional Neural Network (Conv1D CNN)** to perform **time-series analysis** on the dataset.\n",
    "Conv1D is particularly suited for sequential data, as it can capture temporal patterns across the input sequence of features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h2 style=\"text-align:center; font-weight:700;\">ðŸ“š Libraries Used in This Project</h2>\n",
    "<p style=\"text-align:center; color:gray; font-size:15px;\">Core tools used for data processing, modeling, and visualization.</p>\n",
    "<br>\n",
    "<table style=\"margin-left:auto; margin-right:auto; text-align:center;\">\n",
    "  <tr>\n",
    "    <td><img src=\"img/numpy.jpg\" width=\"60\"><br><strong>NumPy</strong><br><span style=\"color:gray; font-size:13px;\">Numerical<br>Computing</span></td>\n",
    "    <td><img src=\"img/pandas.png\" width=\"60\"><br><strong>Pandas</strong><br><span style=\"color:gray; font-size:13px;\">Data<br>Manipulation</span></td>\n",
    "    <td><img src=\"img/pytorch.jpg\" width=\"100\"><br><strong>PyTorch</strong><br><span style=\"color:gray; font-size:13px;\">Deep<br>Learning</span></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"img/scikitlearn.png\" width=\"60\"><br><strong>Scikit-Learn</strong><br><span style=\"color:gray; font-size:13px;\">ML Tools<br>& Metrics</span></td>\n",
    "    <td><img src=\"img/matplotlib.jpg\" width=\"90\"><br><strong>Matplotlib</strong><br><span style=\"color:gray; font-size:13px;\">Visualization</span></td>\n",
    "  </tr>\n",
    "</table>\n",
    "<br>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Thu Oct 23 18:45:00 2025\n",
    "\n",
    "@author: ybenjaminpcondori\n",
    "\"\"\"\n",
    "\n",
    "# System & OS utilities\n",
    "import os\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "#\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_fscore_support\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import (classification_report, f1_score, recall_score,\n",
    "                             confusion_matrix, roc_auc_score, accuracy_score)\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Importing Seaborn for enhanced visualizations\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaring Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Convolutional_Neural_Network(nn.Module):\n",
    "    def __init__(self, num_classes, num_features):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutional feature extractor\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Increased from 64 reâ†’ 128 (better feature extraction)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Slightly reduced from 256 reâ†’ 192 (more stable for tabular data)\n",
    "        self.conv3 = nn.Conv1d(128, 192, kernel_size=3, padding=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        self.conv_dropout = nn.Dropout1d(0.2)\n",
    "\n",
    "        # Determine flatten size dynamically \n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 1, num_features)\n",
    "            d = self._extract_features(dummy)\n",
    "            self.flatten_dim = d.shape[1]\n",
    "\n",
    "        # Classification head \n",
    "        self.fc1 = nn.Linear(self.flatten_dim, 64)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.out = nn.Linear(64, num_classes)\n",
    "\n",
    "    def _extract_features(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.conv_dropout(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.conv_dropout(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.global_pool(x)\n",
    "\n",
    "        return x.squeeze(-1)  # (batch, channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # (batch, 1, features)\n",
    "        x = self._extract_features(x)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        return self.out(x)\n",
    "\n",
    "# Binary counting helper class\n",
    "class DataPreprocessing:\n",
    "\n",
    "    @staticmethod\n",
    "    def count_binary(series):\n",
    "        counts = series.value_counts()\n",
    "        return pd.Series({'0': counts.get(0, 0), '1': counts.get(1, 0)})\n",
    "\n",
    "\n",
    "def random_search(n_trials=30):\n",
    "    best_score = -1.0\n",
    "    best_params = None\n",
    "\n",
    "    for i in range(n_trials):\n",
    "        lr = 10 ** random.uniform(-4, -2.5)\n",
    "        weight_decay = 10 ** random.uniform(-5, -3)\n",
    "        dropout = random.uniform(0.1, 0.35)\n",
    "        cw_scale = random.uniform(0.3, 1.0)\n",
    "\n",
    "        model = Convolutional_Neural_Network(num_classes, num_features).to(device)\n",
    "        model.dropout.p = dropout\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "\n",
    "        weights = class_weights ** cw_scale\n",
    "        criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "        train(model, optimizer, criterion, epochs=20, early_stop_patience=3)\n",
    "        y_true, y_pred = evaluate(model)\n",
    "\n",
    "        score = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = {\n",
    "                \"lr\": lr,\n",
    "                \"weight_decay\": weight_decay,\n",
    "                \"dropout\": dropout,\n",
    "                \"cw_scale\": cw_scale\n",
    "            }\n",
    "\n",
    "    return best_score, best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the CSV Dataset using Pandas\n",
    "df = pd.read_csv(\"IoT_Intrusion.csv\")\n",
    "\n",
    "# Sripping whitespace and converting to lowercase\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "# Displaying the first few rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaration of Columns: Network Protocols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing binary columns\n",
    "protocol_columns = ['http','https','dns','telnet','smtp','ssh','irc','tcp','udp','dhcp','arp','icmp','ipv','llc']\n",
    "for col in protocol_columns:\n",
    "    _ = df[col].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance: Number of instances of 0/1 in each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting binary values in specified columns\n",
    "counts_df = df[protocol_columns].apply(DataPreprocessing.count_binary)\n",
    "print(counts_df)\n",
    "\n",
    "# Printing label statistics\n",
    "print(\"Binary label statistics:\")\n",
    "print(\"Unique label count:\", df['label'].nunique())\n",
    "print(\"Unique labels:\", df['label'].unique())\n",
    "print(\"Label counts:\\n\", df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caclulating Uniques Values, important for identifying rows that don't contribute to the feature prediction/classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Calculating unique values\n",
    "# -----------------------------\n",
    "\n",
    "print(\"\\nUnique Value Counts\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "protocol_columns_nunique = df[protocol_columns].nunique()\n",
    "print(protocol_columns_nunique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaration of Columns: Signal Processing Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to analyze\n",
    "other_columns = [\n",
    "    \"flow_duration\",\n",
    "    \"header_length\",\n",
    "    \"duration\",\n",
    "    \"rate\",\n",
    "    \"srate\",\n",
    "    \"drate\",\n",
    "    \"fin_flag_number\",\n",
    "    \"syn_flag_number\",\n",
    "]\n",
    "\n",
    "# ommited protocol type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null Value Analysis of Other Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null value analysis\n",
    "print(\"Null Value Analysis for Other Columns\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for col in other_columns:\n",
    "    null_count = df[col].isnull().sum()\n",
    "    null_ratio = df[col].isnull().mean()\n",
    "\n",
    "    print(f\"{col:20s} | nulls: {null_count:8d} | ratio: {null_ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Calculating unique values\n",
    "# -----------------------------\n",
    "\n",
    "print(\"\\nUnique Value Counts\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "other_columns_nunique = df[other_columns].nunique()\n",
    "print(other_columns_nunique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaration of Columns: Numeric Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with different names\n",
    "different_columns = [\n",
    "    \"tot size\",\n",
    "    \"magnitue\",\n",
    "    \"iat\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Renaming columns for consistency\n",
    "df = df.rename(columns = {\n",
    "    \"tot size\": \"total_size\",\n",
    "    \"magnitue\": \"magnitude\",\n",
    "    \"iat\": \"inter_arrival_time\",\n",
    "})\n",
    "\n",
    "different_columns = [\"total_size\", \"magnitude\", \"inter_arrival_time\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null value analysis\n",
    "print(\"Null Value Analysis for Different Columns\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for col in different_columns:\n",
    "    null_count = df[col].isnull().sum()\n",
    "    null_ratio = df[col].isnull().mean()\n",
    "\n",
    "    print(f\"{col:20s} | nulls: {null_count:8d} | ratio: {null_ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caclulation Uniques Values, important for identifying rows that don't contribute to the feature prediction/classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Calculating unique values\n",
    "# -----------------------------\n",
    "print(\"\\nUnique Value Counts\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "different_columns_nunique = df[different_columns].nunique()\n",
    "print(different_columns_nunique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting rows with identical features, but different target feature (Label)\n",
    "\n",
    "# Identify feature columns (exclude target)\n",
    "feature_cols = df.columns.difference(['label']).tolist()\n",
    "\n",
    "print(\"Detecting feature-identical rows with multiple target labels\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Detect feature-identical rows with multiple target labels\n",
    "label_variation = (\n",
    "    df\n",
    "    .groupby(feature_cols)['label']\n",
    "    .nunique()\n",
    "    .reset_index(name='n_labels')\n",
    ")\n",
    "\n",
    "# Keep only conflicting feature patterns\n",
    "conflicting_patterns = label_variation[label_variation['n_labels'] > 1]\n",
    "\n",
    "# Retrieve all rows involved in label conflicts\n",
    "df_conflicts = df.merge(\n",
    "    conflicting_patterns[feature_cols],\n",
    "    on=feature_cols,\n",
    "    how='inner',\n",
    ")\n",
    "    # Retrieve all rows involved in label conflicts\n",
    "print(df_conflicts.sort_values(feature_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Detect constant columns across the FULL dataset (excluding label)\n",
    "feature_cols = df.columns.difference(['label']).tolist()\n",
    "\n",
    "global_nunique = df[feature_cols].nunique()\n",
    "\n",
    "constant_cols = global_nunique[global_nunique == 1].index.tolist()\n",
    "\n",
    "print(\"Dropping constant columns:\", constant_cols)\n",
    "\n",
    "df.drop(columns=constant_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missingness Heatmap of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum().sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "label_counts = df[\"label\"].value_counts()\n",
    "\n",
    "ax = sns.barplot(\n",
    "    x=label_counts.index,\n",
    "    y=label_counts.values,\n",
    "    palette=\"Set2\"\n",
    ")\n",
    "\n",
    "# Add numbers on top of bars\n",
    "for i, v in enumerate(label_counts.values):\n",
    "    ax.text(i, v, f\"{v:,}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "plt.title(\"Class Distribution of Attack Labels\", fontsize=14, fontweight=\"bold\")\n",
    "plt.xlabel(\"Attack Type\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = df[\"label\"].value_counts()\n",
    "\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify Features and Target Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "df['label_encoded'] = encoder.fit_transform(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Class to Attack Label Mapping \n",
    "\n",
    "id_to_attack = dict(enumerate(encoder.classes_))\n",
    "attack_to_id = {label: idx for idx, label in id_to_attack.items()}\n",
    "\n",
    "# Displaying the class ID to attack type mapping\n",
    "print(\"\\nClass ID â†’ Attack Type mapping:\\n\")\n",
    "for k, v in id_to_attack.items():\n",
    "    print(f\"Class {k}: {v}\")\n",
    " \n",
    "# Saving the mapping to a CSV file\n",
    "mapping_df = pd.DataFrame({\n",
    "    \"Class ID\": list(id_to_attack.keys()),\n",
    "    \"Attack Type\": list(id_to_attack.values())\n",
    "})\n",
    "\n",
    "# Saving the mapping to a CSV file\n",
    "mapping_df.to_csv(\"class_label_mapping.csv\", index=False)\n",
    "print(\"\\n[âœ“] Saved class_label_mapping.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values\n",
    "target = 'label_encoded'\n",
    "features = [c for c in df.columns if c not in ['label', 'label_encoded']]\n",
    "\n",
    "# Fill missing values in\n",
    "numeric_features = df[features].select_dtypes(include=np.number).columns\n",
    "df[numeric_features] = df[numeric_features].fillna(df[numeric_features].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of X and Y values\n",
    "X = df[features].values\n",
    "y = df[target].values\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Splitting the dataset into training and testing sets with stratification\n",
    "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Initial Training Mean (first 5):\", X_train_np.mean(axis=0)[:5])\n",
    "print(\"Initial Training Std (first 5):\", X_train_np.std(axis=0)[:5])\n",
    "\n",
    "# 4. Preprocessing\n",
    "scaler = StandardScaler()\n",
    "# Fit and transform the training data, overwriting X_train_np\n",
    "X_train_np = scaler.fit_transform(X_train_np)\n",
    "# Transform the test data using the SAME scaler\n",
    "X_test_np  = scaler.transform(X_test_np)\n",
    "\n",
    "# 5. Optional: Verify scaling results\n",
    "# Mean should be ~0, Std should be ~1\n",
    "print(\"Scaled Training Mean:\", X_train_np.mean(axis=0)[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to PyTorch tensors\n",
    "X_train = torch.tensor(X_train_np, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test_np, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train_np, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test_np, dtype=torch.long)\n",
    "\n",
    "# Displaying the shapes of the training and testing tensors\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataLoader based on batch size\n",
    "batch_size = 4096\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class Weights Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(y_train_np),\n",
    "    y=y_train_np\n",
    ")\n",
    "\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "print(\"Class weights:\", class_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model instantiation\n",
    "num_classes = len(df[target].unique())\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "\n",
    "# Convolutional Neural Network model\n",
    "model = Convolutional_Neural_Network(num_classes, num_features).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function Crossentropy Loss\n",
    "loss_function = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Optimizer hyperparameters\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=3e-4\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Learning rate scheduler to reduce LR on plateau\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    " \n",
    "# Training hyperparameters\n",
    "num_epochs = 50\n",
    "\n",
    "# Early Stopping \n",
    "best_val_loss = float(\"inf\")\n",
    "patience = 5          # stop after 5 bad epochs\n",
    "patience_ctr = 0\n",
    "min_delta = 1e-4\n",
    "best_model_path = \"Model\"\n",
    "\n",
    "loss_across_epochs = []\n",
    "val_accuracy_across_epochs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with validation and learning rate scheduling\n",
    "for epoch in range(num_epochs):\n",
    "    # -------- Train --------\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # For looping through training batches\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "        # Training using backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch_x)\n",
    "        loss = loss_function(logits, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss (sum over samples)\n",
    "        total_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "        # Accumulate accuracy\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == batch_y).sum().item()\n",
    "        total += batch_y.size(0)\n",
    "\n",
    "    # Calculating average training loss/accuracy\n",
    "    train_loss = total_loss / len(train_loader.dataset)\n",
    "    train_acc = correct / total if total > 0 else 0.0\n",
    "\n",
    "    # -------- Validation (using test_loader as validation set) --------\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for bx, by in test_loader:\n",
    "            bx, by = bx.to(device), by.to(device)\n",
    "            logits = model(bx)\n",
    "            val_loss += loss_function(logits, by).item() * bx.size(0)\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            val_correct += (preds == by).sum().item()\n",
    "            val_total += by.size(0)\n",
    "\n",
    "    # Calculating average validation loss/accuracy\n",
    "    val_loss /= len(test_loader.dataset)\n",
    "    val_acc = val_correct / val_total if val_total > 0 else 0.0\n",
    "\n",
    "    loss_across_epochs.append(train_loss)\n",
    "    val_accuracy_across_epochs.append(val_acc)\n",
    "\n",
    "\n",
    "    # Scheduler step\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Printing Training progress\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "        f\"Train Acc: {train_acc:.4f} | \"\n",
    "        f\"Train Loss: {train_loss:.4f} | \"\n",
    "        f\"Val Acc: {val_acc:.4f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f} | \"\n",
    "        f\"LR: {optimizer.param_groups[0]['lr']:.2e}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Formatting for Presentation purposes on Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib global settings\n",
    "mpl.rcParams.update({\n",
    "    # Resolution\n",
    "    \"figure.dpi\": 150,\n",
    "    \"savefig.dpi\": 300,\n",
    "\n",
    "    # Font (IEEE/Springer safe)\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Times New Roman\", \"Times\", \"DejaVu Serif\"],\n",
    "\n",
    "    # Font sizes\n",
    "    \"font.size\": 10,\n",
    "    \"axes.titlesize\": 12,\n",
    "    \"axes.titleweight\": \"bold\",\n",
    "    \"axes.labelsize\": 11,\n",
    "    \"axes.labelweight\": \"bold\",\n",
    "\n",
    "    \"xtick.labelsize\": 9,\n",
    "    \"ytick.labelsize\": 9,\n",
    "\n",
    "    \"legend.fontsize\": 9,\n",
    "    \"legend.frameon\": False,\n",
    "\n",
    "    # Clean axes\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.spines.right\": False,\n",
    "    \"axes.grid\": False,\n",
    "\n",
    "    # Line widths\n",
    "    \"lines.linewidth\": 2,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "\n",
    "all_true = []\n",
    "all_preds = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bx, by in test_loader:\n",
    "        bx = bx.to(device)\n",
    "        by = by.to(device)\n",
    "\n",
    "        logits = model(bx)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "        all_true.extend(by.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_true = np.array(all_true)\n",
    "all_preds = np.array(all_preds)\n",
    "all_probs = np.array(all_probs)\n",
    "\n",
    "print(\"Evaluation tensors shapes:\")\n",
    "print(\"all_true :\", all_true.shape)\n",
    "print(\"all_preds:\", all_preds.shape)\n",
    "print(\"all_probs:\", all_probs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluation metrics with attack namess\n",
    "\n",
    "def evaluate_with_names(all_true, all_preds, all_probs, id_to_attack):\n",
    "    num_classes = len(id_to_attack)\n",
    "    class_names = [id_to_attack[i] for i in range(num_classes)]\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"MODEL EVALUATION METRICS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(\n",
    "        all_true,\n",
    "        all_preds,\n",
    "        target_names=class_names,\n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "    macro_f1 = f1_score(all_true, all_preds, average='macro', zero_division=0)\n",
    "    weighted_f1 = f1_score(all_true, all_preds, average='weighted', zero_division=0)\n",
    "\n",
    "    print(\"\\nF1 Scores:\")\n",
    "    print(f\"  Macro F1 Score:     {macro_f1:.4f}\")\n",
    "    print(f\"  Weighted F1 Score:  {weighted_f1:.4f}\")\n",
    "\n",
    "    macro_recall = recall_score(all_true, all_preds, average='macro', zero_division=0)\n",
    "    weighted_recall = recall_score(all_true, all_preds, average='weighted', zero_division=0)\n",
    "    per_class_recall = recall_score(all_true, all_preds, average=None, zero_division=0)\n",
    "\n",
    "    print(\"\\nRecall Scores:\")\n",
    "    print(f\"  Macro Recall:     {macro_recall:.4f}\")\n",
    "    print(f\"  Weighted Recall:  {weighted_recall:.4f}\")\n",
    "    print(\"  Per-class Recall:\")\n",
    "    for i, r in enumerate(per_class_recall):\n",
    "        print(f\"    {id_to_attack[i]}: {r:.4f}\")\n",
    "\n",
    "    cm = confusion_matrix(all_true, all_preds)\n",
    "    accuracy = accuracy_score(all_true, all_preds)\n",
    "\n",
    "    print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    return {\n",
    "        'macro_f1': macro_f1,\n",
    "        'weighted_f1': weighted_f1,\n",
    "        'macro_recall': macro_recall,\n",
    "        'weighted_recall': weighted_recall,\n",
    "        'accuracy': accuracy,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "def evaluation_model_metrics(all_true, all_preds, id_to_attack, normalize=True):\n",
    "    \"\"\"\n",
    "    Plots confusion matrix (raw + normalized) and returns per-class recall.\n",
    "    \"\"\"\n",
    "\n",
    "    class_names = [id_to_attack[i] for i in range(len(id_to_attack))]\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_true, all_preds)\n",
    "\n",
    "    # Normalised confusion matrix\n",
    "    if normalize:\n",
    "        cm_norm = cm.astype(\"float\") / cm.sum(axis=1, keepdims=True)\n",
    "        cm_norm = np.nan_to_num(cm_norm)\n",
    "    else:\n",
    "        cm_norm = None\n",
    "\n",
    "    # Per-class recall\n",
    "    per_class_recall = recall_score(\n",
    "        all_true,\n",
    "        all_preds,\n",
    "        average=None,\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    # ---------- Plot ----------\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # Raw CM\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        ax=axes[0],\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=class_names,\n",
    "        yticklabels=class_names,\n",
    "        cbar=False\n",
    "    )\n",
    "    axes[0].set_title(\"Confusion Matrix\")\n",
    "    axes[0].set_xlabel(\"Predicted\")\n",
    "    axes[0].set_ylabel(\"True\")\n",
    "\n",
    "    # Normalised CM\n",
    "    sns.heatmap(\n",
    "        cm_norm,\n",
    "        ax=axes[1],\n",
    "        cmap=\"Greens\",\n",
    "        xticklabels=class_names,\n",
    "        yticklabels=class_names,\n",
    "        cbar=True,\n",
    "        vmin=0,\n",
    "        vmax=1\n",
    "    )\n",
    "    axes[1].set_title(\"Normalised Confusion Matrix\")\n",
    "    axes[1].set_xlabel(\"Predicted\")\n",
    "    axes[1].set_ylabel(\"True\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"confusion_matrix_normalised\": cm_norm,\n",
    "        \"per_class_recall\": per_class_recall\n",
    "    }\n",
    "\n",
    "# ROC-AUC curves and metrics heatmap with attack names\n",
    "\n",
    "def plot_roc_and_heatmap(all_true, all_probs, id_to_attack,\n",
    "                          max_classes=5,\n",
    "                          save_path='roc_and_metrics_heatmap.png'):\n",
    "    num_classes = len(id_to_attack)\n",
    "    class_names = [id_to_attack[i] for i in range(num_classes)]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # ROC curves (OvR)\n",
    "    all_true_bin = label_binarize(all_true, classes=range(num_classes))\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, num_classes))\n",
    "\n",
    "    for i in range(min(num_classes, max_classes)):\n",
    "        fpr, tpr, _ = roc_curve(all_true_bin[:, i], all_probs[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        axes[0].plot(fpr, tpr, lw=2,\n",
    "                     label=f'{class_names[i]} (AUC={roc_auc:.3f})')\n",
    "\n",
    "    axes[0].plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    axes[0].set_xlabel('False Positive Rate')\n",
    "    axes[0].set_ylabel('True Positive Rate')\n",
    "    axes[0].set_title('ROC Curves (One-vs-Rest)')\n",
    "    axes[0].legend(fontsize=8)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Metrics heatmap\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_true, np.argmax(all_probs, axis=1), average=None, zero_division=0\n",
    "    )\n",
    "    metrics_data = np.vstack([precision, recall, f1]).T\n",
    "\n",
    "    im = axes[1].imshow(metrics_data[:max_classes], cmap='RdYlGn', vmin=0, vmax=1)\n",
    "    axes[1].set_xticks([0, 1, 2])\n",
    "    axes[1].set_xticklabels(['Precision', 'Recall', 'F1'])\n",
    "    axes[1].set_yticks(range(min(num_classes, max_classes)))\n",
    "    axes[1].set_yticklabels(class_names[:max_classes])\n",
    "    axes[1].set_title('Per-Class Metrics Heatmap')\n",
    "\n",
    "    plt.colorbar(im, ax=axes[1])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Gathering all predictions and probabilities\n",
    "\n",
    "\n",
    "metrics = evaluate_with_names(\n",
    "    all_true,\n",
    "    all_preds,\n",
    "    all_probs,\n",
    "    id_to_attack\n",
    ")\n",
    "\n",
    "macro_f1 = metrics[\"macro_f1\"]\n",
    "weighted_f1 = metrics[\"weighted_f1\"]\n",
    "macro_recall = metrics[\"macro_recall\"]\n",
    "accuracy = metrics[\"accuracy\"]\n",
    "\n",
    "\n",
    "\n",
    "viz_metrics = evaluation_model_metrics(\n",
    "    all_true,\n",
    "    all_preds,\n",
    "    id_to_attack,\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "cm = viz_metrics[\"confusion_matrix\"]\n",
    "cm_normalized = viz_metrics[\"confusion_matrix_normalised\"]\n",
    "per_class_recall = viz_metrics[\"per_class_recall\"]\n",
    "\n",
    "\n",
    "# Additional metrics visualization\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 1. F1 Scores Comparison\n",
    "ax1 = axes[0]\n",
    "metrics = ['Macro F1', 'Weighted F1']\n",
    "scores = [macro_f1, weighted_f1]\n",
    "colors_metrics = ['#FF6B6B', '#4ECDC4']\n",
    "bars = ax1.bar(metrics, scores, color=colors_metrics, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax1.set_ylabel('F1 Score', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('F1 Scores Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, score in zip(bars, scores):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{score:.4f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 2. Macro Metrics Summary\n",
    "ax2 = axes[1]\n",
    "macro_precision = precision_score(all_true, all_preds, average='macro', zero_division=0)\n",
    "summary_metrics = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
    "summary_values = [accuracy, macro_precision, macro_recall, macro_f1]\n",
    "colors_summary = ['#95E1D3', '#F38181', '#AA96DA', '#FCBAD3']\n",
    "\n",
    "bars = ax2.barh(summary_metrics, summary_values, color=colors_summary, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax2.set_xlabel('Score', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Macro-Averaged Metrics Summary', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlim([0, 1]) \n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "for bar, value in zip(bars, summary_values):\n",
    "    width = bar.get_width()\n",
    "    ax2.text(width, bar.get_y() + bar.get_height()/2.,\n",
    "             f'{value:.4f}', ha='left', va='center', fontsize=11, fontweight='bold', \n",
    "             bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('metrics_summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\" Metrics summary visualization saved as 'metrics_summary.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC-AUC Curves (One-vs-Rest for multi-class)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# 1. ROC Curve (One-vs-Rest)\n",
    "ax1 = axes[0]\n",
    "if num_classes > 2:\n",
    "    # Multi-class: use label binarization\n",
    "    all_true_bin = label_binarize(all_true, classes=range(num_classes))\n",
    "    \n",
    "    colors = plt.cm.Set3(np.linspace(0, 2, num_classes))\n",
    "    auc_scores = []\n",
    "    \n",
    "    for i in range(min(num_classes, 5)):  # Limit to 5 classes for clarity\n",
    "        fpr, tpr, _ = roc_curve(all_true_bin[:, i], all_probs[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        auc_scores.append(roc_auc)\n",
    "        ax1.plot(fpr, tpr, color=colors[i], lw=2, label=f'Class {i} (AUC = {roc_auc:.3f})')\n",
    "    \n",
    "    # Micro-average\n",
    "    fpr, tpr, _ = roc_curve(all_true_bin.ravel(), all_probs.ravel())\n",
    "    roc_auc_micro = auc(fpr, tpr)\n",
    "    ax1.plot(fpr, tpr, color='deeppink', lw=4, linestyle=':', label=f'Micro-average (AUC = {roc_auc_micro:.3f})')\n",
    "    \n",
    "    ax1.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
    "    ax1.set_xlim([0.0, 1.0])\n",
    "    ax1.set_ylim([0.0, 1.05])\n",
    "    ax1.set_xlabel('False Positive Rate', fontsize=11, fontweight='bold')\n",
    "    ax1.set_ylabel('True Positive Rate', fontsize=11, fontweight='bold')\n",
    "    ax1.set_title('ROC Curves (One-vs-Rest) - Top 5 Classes', fontsize=13, fontweight='bold')\n",
    "    ax1.legend(loc=\"lower right\", fontsize=9)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "else:\n",
    "    # Binary classification\n",
    "    fpr, tpr, _ = roc_curve(all_true, all_probs[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax1.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "    ax1.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
    "    ax1.set_xlim([0.0, 1.0])\n",
    "    ax1.set_ylim([0.0, 1.05])\n",
    "    ax1.set_xlabel('False Positive Rate', fontsize=11, fontweight='bold')\n",
    "    ax1.set_ylabel('True Positive Rate', fontsize=11, fontweight='bold')\n",
    "    ax1.set_title('ROC Curve', fontsize=13, fontweight='bold')\n",
    "    ax1.legend(loc=\"lower right\", fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Per-Class Metrics Heatmap\n",
    "ax2 = axes[1]\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision, recall, f1, support = precision_recall_fscore_support(all_true, all_preds, \n",
    "                                                                   average=None, zero_division=0)\n",
    "\n",
    "metrics_data = np.array([precision, recall, f1]).T\n",
    "im = ax2.imshow(metrics_data, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "\n",
    "# Set ticks and labels\n",
    "ax2.set_xticks([0, 1, 2])\n",
    "ax2.set_xticklabels(['Precision', 'Recall', 'F1'], fontsize=11, fontweight='bold')\n",
    "ax2.set_yticks(range(min(len(precision), 10)))\n",
    "ax2.set_yticklabels([f'Class {i}' for i in range(min(len(precision), 10))], fontsize=10)\n",
    "ax2.set_title('Per-Class Metrics Heatmap (Top 10 Classes)', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(min(len(precision), 10)):\n",
    "    for j in range(3):\n",
    "        text = ax2.text(j, i, f'{metrics_data[i, j]:.2f}',\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontsize=9, fontweight='bold')\n",
    "\n",
    "cbar = plt.colorbar(im, ax=ax2)\n",
    "cbar.set_label('Score', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_and_metrics_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\" ROC and metrics heatmap visualization saved as 'roc_and_metrics_heatmap.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with multiple subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Confusion Matrix Heatmap\n",
    "ax1 = axes[0, 0]\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm, annot=False, fmt='d', cmap='Blues', ax=ax1, cbar_kws={'label': 'Count'})\n",
    "ax1.set_title('Confusion Matrix (Raw Counts)', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('True Label', fontsize=12)\n",
    "ax1.set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "# 2. Normalized Confusion Matrix\n",
    "ax2 = axes[0, 1]\n",
    "sns.heatmap(cm_normalized, annot=False, fmt='.2%', cmap='RdYlGn', ax=ax2, cbar_kws={'label': 'Recall %'})\n",
    "ax2.set_title('Confusion Matrix (Normalized by True Label)', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('True Label', fontsize=12)\n",
    "ax2.set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "# 3. Training Loss and Validation Accuracy\n",
    "ax3 = axes[1, 0]\n",
    "epochs_range = range(1, len(loss_across_epochs) + 1)\n",
    "ax3_twin = ax3.twinx()\n",
    "line1 = ax3.plot(epochs_range, loss_across_epochs, 'b-o', label='Training Loss', linewidth=2, markersize=6)\n",
    "line2 = ax3_twin.plot(epochs_range, val_accuracy_across_epochs, 'g-s', label='Validation Accuracy', linewidth=2, markersize=6)\n",
    "ax3.set_xlabel('Epoch', fontsize=12)\n",
    "ax3.set_ylabel('Loss', fontsize=12, color='b')\n",
    "ax3_twin.set_ylabel('Accuracy', fontsize=12, color='g')\n",
    "ax3.tick_params(axis='y', labelcolor='b')\n",
    "ax3_twin.tick_params(axis='y', labelcolor='g')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_title('Training Loss vs Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Combine legends\n",
    "lines = line1 + line2\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax3.legend(lines, labels, loc='upper left', fontsize=10)\n",
    "\n",
    "# 4. Per-Class Recall Bar Chart\n",
    "ax4 = axes[1, 1]\n",
    "class_names = [id_to_attack[i] for i in range(len(per_class_recall))]\n",
    "\n",
    "colors = ['green' if r > 0.5 else 'orange' if r > 0.3 else 'red' for r in per_class_recall]\n",
    "bars = ax4.bar(class_names, per_class_recall, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "ax4.axhline(y=macro_recall, color='r', linestyle='--', linewidth=2, label=f'Macro Recall: {macro_recall:.4f}')\n",
    "ax4.set_ylabel('Recall', fontsize=12)\n",
    "ax4.set_title('Per-Class Recall (Detection Rate)', fontsize=14, fontweight='bold')\n",
    "ax4.set_ylim([0, 1])\n",
    "ax4.legend(fontsize=10)\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, recall in zip(bars, per_class_recall):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{recall:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_evaluation_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Visualizations saved as 'model_evaluation_metrics.png'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
