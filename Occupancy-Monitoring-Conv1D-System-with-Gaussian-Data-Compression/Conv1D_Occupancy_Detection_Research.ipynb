{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50536fb4",
   "metadata": {},
   "source": [
    "# Conv1D-Based Occupancy Detection from Multisensor Time-Series Data\n",
    "\n",
    "## Abstract\n",
    "This notebook presents an end-to-end Conv1D pipeline for occupancy detection using multisensor environmental time-series data. It is structured for research, reproducibility, and publication-ready reporting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2099058",
   "metadata": {},
   "source": [
    "## 1. Imports and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b8ea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report,\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, log_loss, roc_curve\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaac42b",
   "metadata": {},
   "source": [
    "## 2. Dataset Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d884c83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(r\"C:/Users/ybenj/Downloads/ALL_CSVS/merged_env_motion.csv\")\n",
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "df = df[df['motion'].notna()]\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
    "\n",
    "columns_to_drop = ['smoke', 'device']\n",
    "if 'co' in df.columns:\n",
    "    columns_to_drop.append('co')\n",
    "df.drop(columns=columns_to_drop, errors='ignore', inplace=True)\n",
    "\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp']).astype('int64') / 1e9\n",
    "\n",
    "if 'light' in df.columns and df['light'].dtype == bool:\n",
    "    df['light'] = df['light'].astype(int)\n",
    "\n",
    "df.drop(columns=['hub', 'home'], inplace=True, errors='ignore')\n",
    "\n",
    "target = 'motion'\n",
    "df[target] = df[target].astype(int)\n",
    "features = [col for col in df.columns if col != target]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[features] = scaler.fit_transform(df[features])\n",
    "\n",
    "X = df[features].values.astype(np.float32)\n",
    "y = df[target].values.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b60a4c1",
   "metadata": {},
   "source": [
    "## 3. Sliding Window Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc95a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_windows(data, labels, window_size):\n",
    "    X_windows, y_labels = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X_windows.append(data[i:i+window_size])\n",
    "        y_labels.append(labels[i+window_size])\n",
    "    return np.array(X_windows), np.array(y_labels)\n",
    "\n",
    "TIME_STEPS = 100\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "X_windowed, y_windowed = create_windows(X, y, TIME_STEPS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb2de29",
   "metadata": {},
   "source": [
    "## 4. TensorFlow Dataset Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2891c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_windowed, y_windowed))\n",
    "dataset = dataset.shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "total_batches = len(list(dataset))\n",
    "train_size = int(0.8 * total_batches)\n",
    "\n",
    "train_ds = dataset.take(train_size)\n",
    "test_ds = dataset.skip(train_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3037d4",
   "metadata": {},
   "source": [
    "## 5. Class Imbalance Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbfa12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_weights_array = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_windowed.astype(int)),\n",
    "    y=y_windowed.astype(int)\n",
    ")\n",
    "\n",
    "class_weights = dict(enumerate(class_weights_array))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b96f367",
   "metadata": {},
   "source": [
    "## 6. Conv1D Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113f9b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(TIME_STEPS, len(features))),\n",
    "    tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling1D(2),\n",
    "\n",
    "    tf.keras.layers.Conv1D(128, 3, activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling1D(2),\n",
    "\n",
    "    tf.keras.layers.Conv1D(256, 3, activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling1D(2),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb85a07",
   "metadata": {},
   "source": [
    "## 7. Training with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a649703",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=10,\n",
    "    validation_data=test_ds,\n",
    "    callbacks=[early_stop],\n",
    "    class_weight=class_weights\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae46065",
   "metadata": {},
   "source": [
    "## 8. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f04a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbad84e",
   "metadata": {},
   "source": [
    "## 9. Metrics and Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d42bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_true, y_pred_probs, y_pred_classes = [], [], []\n",
    "\n",
    "for x_batch, y_batch in test_ds:\n",
    "    preds = model.predict(x_batch).flatten()\n",
    "    y_true.extend(y_batch.numpy())\n",
    "    y_pred_probs.extend(preds)\n",
    "    y_pred_classes.extend((preds > 0.5).astype(int))\n",
    "\n",
    "y_true = np.array(y_true).astype(int)\n",
    "y_pred_probs = np.array(y_pred_probs)\n",
    "y_pred_classes = np.array(y_pred_classes)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
